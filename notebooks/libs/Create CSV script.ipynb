{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weenkus/anaconda3/envs/py27/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import FeatureExtraction as FE\n",
    "import numpy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sexual_predator_ids_file = '../../dataset/test/pan12-sexual-predator-identification-groundtruth-problem1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_csv(input_file_path, output_file_name, batch_size):\n",
    "    tree = etree.parse(input_file_path)\n",
    "    author_conversation_node_dictionary = FE.extract_author_conversation_node_dictionary_from_XML(tree)\n",
    "    del tree\n",
    "    \n",
    "    output_file_csv = open(output_file_name, 'w+')\n",
    "    output_string_list = ['autor', 'number of conversation', 'percent of conversations started by the author',\n",
    "                         'difference between two preceding lines in seconds', 'number of messages sent',\n",
    "                         'average percent of lines in conversation', 'average percent of characters in conversation',\n",
    "                         'number of characters sent by the author', 'mean time of messages sent',\n",
    "                         'number of unique contacted authors', 'avg number of unique authors interacted with per conversation',\n",
    "                         'total unique authors and unique per chat difference',\n",
    "                         'conversation num and total unique authors difference',\n",
    "                         'average question marks per conversations', 'total question marks', 'total author question marks',\n",
    "                         'avg author question marks', 'author and conversation quetsion mark differnece',\n",
    "                         #'total negative in author conv', 'total neutral in author conv', 'total positive in author conv',\n",
    "                         #'total compound in author conv',\n",
    "                          #'author total negative in author conv', \n",
    "                         #'author total neutral in author conv', 'author total positive in author conv',\n",
    "                         #' authortotal compound in author conv',\n",
    "                          #'conversation and author negative differnece', \n",
    "                         #'conversation and author neutral differnece', 'conversation and author positive differnece',\n",
    "                         #' conversation and author compound differnece',\n",
    "                          'pos word count author','neg word count author',\n",
    "                          'is sexual predator']\n",
    "    output_string = ','.join(output_string_list) + \"\\n\"\n",
    "    \n",
    "    sexual_predator_ids_list = FE.sexual_predator_ids(sexual_predator_ids_file)\n",
    "    \n",
    "    i = 0\n",
    "    for index, author in enumerate(sorted(author_conversation_node_dictionary)):\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            print index, len(author_conversation_node_dictionary)\n",
    "        \n",
    "        conversation_nodes = author_conversation_node_dictionary[author]\n",
    "        conversation_nodes_length = len(conversation_nodes)\n",
    "        \n",
    "        #conversation_text_sentiment_total = FE.calculate_conversation_sentiment_total(\n",
    "        #    author, conversation_nodes)\n",
    "        \n",
    "        #author_conversation_text_sentiment_total = FE.calculate_author_conversation_sentiment_total(author, conversation_nodes)\n",
    "        \n",
    "        total_unique_authors = FE.number_of_unique_authors_interacted_with(author, conversation_nodes)\n",
    "        total_author_question_marks = FE.total_authors_question_marks_per_conversation(author, conversation_nodes)\n",
    "        \n",
    "        output_list = [author,\n",
    "                       len(conversation_nodes),\n",
    "                       FE.average_trough_all_conversations(author, conversation_nodes, FE.is_starting_conversation),\n",
    "                       FE.average_trough_all_conversations(author, conversation_nodes, \n",
    "                                    FE.avg_time_between_message_lines_in_seconds_for_author_in_conversation),\n",
    "                       FE.number_of_messages_sent_by_the_author(author, conversation_nodes),\n",
    "                       FE.average_trough_all_conversations(author, conversation_nodes,\n",
    "                                                           FE.percentage_of_lines_in_conversation),\n",
    "                       FE.average_trough_all_conversations(author, conversation_nodes,\n",
    "                                                           FE.percentage_of_characters_in_conversation),\n",
    "                       FE.number_of_characters_sent_by_the_author(author, conversation_nodes),\n",
    "                       FE.mean_time_of_messages_sent(author, conversation_nodes),\n",
    "                       total_unique_authors,\n",
    "                       total_unique_authors/conversation_nodes_length,\n",
    "                       FE.difference_unique_authors_per_chat_and_total_unique(\n",
    "                           total_unique_authors, total_unique_authors/conversation_nodes_length),\n",
    "                       FE.difference_unique_authors_and_conversations(\n",
    "                           total_unique_authors, conversation_nodes_length\n",
    "                        ),\n",
    "                       FE.avg_question_marks_per_conversation(author, conversation_nodes),\n",
    "                       FE.total_question_marks_per_conversation(author, conversation_nodes),\n",
    "                       total_author_question_marks,\n",
    "                       total_author_question_marks/conversation_nodes_length,\n",
    "                       abs(total_author_question_marks - FE.total_question_marks_per_conversation(author, conversation_nodes)),\n",
    "                       #conversation_text_sentiment_total['neg'],\n",
    "                       #conversation_text_sentiment_total['neu'],\n",
    "                       #conversation_text_sentiment_total['pos'],\n",
    "                       #conversation_text_sentiment_total['compound'],\n",
    "                       #author_conversation_text_sentiment_total['neg'],\n",
    "                       #author_conversation_text_sentiment_total['neu'],\n",
    "                       #author_conversation_text_sentiment_total['pos'],\n",
    "                       #author_conversation_text_sentiment_total['compound'],\n",
    "                       #abs(conversation_text_sentiment_total['neg']-author_conversation_text_sentiment_total['neg']),\n",
    "                       #abs(conversation_text_sentiment_total['neu']-author_conversation_text_sentiment_total['neu']),\n",
    "                       #abs(conversation_text_sentiment_total['pos']-author_conversation_text_sentiment_total['pos']),\n",
    "                       #abs(conversation_text_sentiment_total['compound']-author_conversation_text_sentiment_total['compound']),\n",
    "                       positive_count_author(author, conversation_nodes),\n",
    "                       negative_count_author(author, conversation_nodes),\n",
    "                       '1' if author in sexual_predator_ids_list else '0'\n",
    "                      ]\n",
    "        output_string += ','.join(map(str, output_list)) + '\\n'\n",
    "        if i == batch_size:\n",
    "            output_file_csv.write(output_string)\n",
    "            output_string = ''\n",
    "            i = -1\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    output_file_csv.write(output_string)    \n",
    "    del output_string\n",
    "    del author_conversation_node_dictionary\n",
    "    output_file_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path='../../dataset/test/pan12-sexual-predator-identification-test-corpus-2012-05-17.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 218702\n",
      "100 218702\n",
      "200 218702\n",
      "300 218702\n",
      "400 218702\n",
      "500 218702\n",
      "600 218702\n",
      "700 218702\n",
      "800 218702\n",
      "900 218702\n",
      "1000 218702\n",
      "1100 218702\n",
      "1200 218702\n",
      "1300 218702\n",
      "1400 218702\n",
      "1500 218702\n",
      "1600 218702\n",
      "1700 218702\n",
      "1800 218702\n",
      "1900 218702\n",
      "2000 218702\n",
      "2100 218702\n",
      "2200 218702\n",
      "2300 218702\n",
      "2400 218702\n",
      "2500 218702\n",
      "2600 218702\n",
      "2700 218702\n",
      "2800 218702\n",
      "2900 218702\n",
      "3000 218702\n",
      "3100 218702\n",
      "3200 218702\n",
      "3300 218702\n",
      "3400 218702\n",
      "3500 218702\n",
      "3600 218702\n",
      "3700 218702\n",
      "3800 218702\n",
      "3900 218702\n",
      "4000 218702\n",
      "4100 218702\n",
      "4200 218702\n",
      "4300 218702\n",
      "4400 218702\n",
      "4500 218702\n",
      "4600 218702\n",
      "4700 218702\n",
      "4800 218702\n",
      "4900 218702\n",
      "5000 218702\n",
      "5100 218702\n",
      "5200 218702\n",
      "5300 218702\n",
      "5400 218702\n",
      "5500 218702\n",
      "5600 218702\n",
      "5700 218702\n",
      "5800 218702\n",
      "5900 218702\n",
      "6000 218702\n",
      "6100 218702\n",
      "6200 218702\n",
      "6300 218702\n",
      "6400 218702\n",
      "6500 218702\n",
      "6600 218702\n",
      "6700 218702\n",
      "6800 218702\n",
      "6900 218702\n",
      "7000 218702\n",
      "7100 218702\n",
      "7200 218702\n",
      "7300 218702\n",
      "7400 218702\n",
      "7500 218702\n",
      "7600 218702\n",
      "7700 218702\n",
      "7800 218702\n",
      "7900 218702\n",
      "8000 218702\n",
      "8100 218702\n",
      "8200 218702\n",
      "8300 218702\n",
      "8400 218702\n",
      "8500 218702\n",
      "8600 218702\n",
      "8700 218702\n",
      "8800 218702\n",
      "8900 218702\n",
      "9000 218702\n",
      "9100 218702\n",
      "9200 218702\n",
      "9300 218702\n",
      "9400 218702\n",
      "9500 218702\n",
      "9600 218702\n",
      "9700 218702\n",
      "9800 218702\n",
      "9900 218702\n",
      "10000 218702\n",
      "10100 218702\n",
      "10200 218702\n",
      "10300 218702\n",
      "10400 218702\n",
      "10500 218702\n",
      "10600 218702\n",
      "10700 218702\n",
      "10800 218702\n",
      "10900 218702\n",
      "11000 218702\n",
      "11100 218702\n",
      "11200 218702\n",
      "11300 218702\n",
      "11400 218702\n",
      "11500 218702\n",
      "11600 218702\n",
      "11700 218702\n",
      "11800 218702\n",
      "11900 218702\n",
      "12000 218702\n",
      "12100 218702\n",
      "12200 218702\n",
      "12300 218702\n",
      "12400 218702\n",
      "12500 218702\n",
      "12600 218702\n",
      "12700 218702\n",
      "12800 218702\n",
      "12900 218702\n",
      "13000 218702\n",
      "13100 218702\n",
      "13200 218702\n",
      "13300 218702\n",
      "13400 218702\n",
      "13500 218702\n",
      "13600 218702\n",
      "13700 218702\n",
      "13800 218702\n",
      "13900 218702\n",
      "14000 218702\n",
      "14100 218702\n",
      "14200 218702\n",
      "14300 218702\n",
      "14400 218702\n",
      "14500 218702\n",
      "14600 218702\n",
      "14700 218702\n",
      "14800 218702\n",
      "14900 218702\n",
      "15000 218702\n",
      "15100 218702\n",
      "15200 218702\n",
      "15300 218702\n",
      "15400 218702\n",
      "15500 218702\n",
      "15600 218702\n",
      "15700 218702\n",
      "15800 218702\n",
      "15900 218702\n",
      "16000 218702\n",
      "16100 218702\n",
      "16200 218702\n",
      "16300 218702\n",
      "16400 218702\n",
      "16500 218702\n",
      "16600 218702\n",
      "16700 218702\n",
      "16800 218702\n",
      "16900 218702\n",
      "17000 218702\n",
      "17100 218702\n",
      "17200 218702\n",
      "17300 218702\n",
      "17400 218702\n",
      "17500 218702\n",
      "17600 218702\n",
      "17700 218702\n",
      "17800 218702\n",
      "17900 218702\n",
      "18000 218702\n",
      "18100 218702\n",
      "18200 218702\n",
      "18300 218702\n",
      "18400 218702\n",
      "18500 218702\n",
      "18600 218702\n",
      "18700 218702\n",
      "18800 218702\n",
      "18900 218702\n",
      "19000 218702\n",
      "19100 218702\n",
      "19200 218702\n",
      "19300 218702\n",
      "19400 218702\n",
      "19500 218702\n",
      "19600 218702\n",
      "19700 218702\n",
      "19800 218702\n",
      "19900 218702\n",
      "20000 218702\n",
      "20100 218702\n",
      "20200 218702\n",
      "20300 218702\n",
      "20400 218702\n",
      "20500 218702\n",
      "20600 218702\n",
      "20700 218702\n",
      "20800 218702\n",
      "20900 218702\n",
      "21000 218702\n",
      "21100 218702\n",
      "21200 218702\n",
      "21300 218702\n",
      "21400 218702\n",
      "21500 218702\n",
      "21600 218702\n",
      "21700 218702\n",
      "21800 218702\n",
      "21900 218702\n",
      "22000 218702\n",
      "22100 218702\n",
      "22200 218702\n",
      "22300 218702\n",
      "22400 218702\n",
      "22500 218702\n",
      "22600 218702\n",
      "22700 218702\n",
      "22800 218702\n",
      "22900 218702\n",
      "23000 218702\n",
      "23100 218702\n",
      "23200 218702\n",
      "23300 218702\n",
      "23400 218702\n",
      "23500 218702\n",
      "23600 218702\n",
      "23700 218702\n"
     ]
    }
   ],
   "source": [
    "create_csv(file_path, '../../csv/chat_based_features_test.csv', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sid = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "sentiment = sid.polarity_scores(\"What is this sentiment I must do?!!\")\n",
    "print sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_file = open('positive.txt', 'r')\n",
    "neg_file = open('negative.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def positive_count_author(author, conversation_nodes):\n",
    "    pos_file = open('positive.txt', 'r')\n",
    "    pos_words = set(pos_file.read().split('\\n'))\n",
    "    whole_text = ''\n",
    "    \n",
    "    for conversation_node in conversation_nodes:\n",
    "        \n",
    "        if len(conversation_node.xpath('.//message//text')) == 0:\n",
    "            continue\n",
    "        \n",
    "        for message in FE.message_texts_of_author_in_conversation(conversation_node, author):\n",
    "            \n",
    "            if message is not None:\n",
    "                whole_text += message\n",
    "\n",
    "    \n",
    "    words = set(whole_text.split(' '))\n",
    "    return len(words.intersection(pos_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_count_author(author, conversation_nodes):\n",
    "    neg_file = open('negative.txt', 'r')\n",
    "    neg_words = set(neg_file.read().split('\\n'))\n",
    "    whole_text = ''\n",
    "    \n",
    "    for conversation_node in conversation_nodes:\n",
    "        \n",
    "        if len(conversation_node.xpath('.//message//text')) == 0:\n",
    "            continue\n",
    "        \n",
    "        for message in FE.message_texts_of_author_in_conversation(conversation_node, author):\n",
    "            \n",
    "            if message is not None:\n",
    "                whole_text += message\n",
    "\n",
    "        \n",
    "    words = set(whole_text.split(' '))\n",
    "    return len(words.intersection(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
